{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2f3d5b6",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936daafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.5)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (2.28.2)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (4.65.0)\n",
      "Requirement already satisfied: packaging in /Users/sachin/Library/Python/3.11/lib/python/site-packages (from webdriver-manager) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.8.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (22.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.1.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install webdriver-manager\n",
    "%pip install selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver         #we use selenium for automation of the programs\n",
    "import pandas as pd                        #to read the dataframe into csv\n",
    "import csv                                     #to import the data into csv\n",
    "from selenium.webdriver.common.keys import Keys                                                            #we need all these modules for all question so imported in 1st line\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep        #time we need for website to load\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14aae6a8",
   "metadata": {},
   "source": [
    "## Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ffc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product = input(\"Enter the product you want to search\")  #Ask the user for input\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in\")  #Navigate to Amazon Website\n",
    "\n",
    "search_box = driver.find_element(\"id\", 'twotabsearchtextbox')\n",
    "search_box.send_keys(product)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e5a08e",
   "metadata": {},
   "source": [
    "## In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b63027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv   #to store the results in csv\n",
    "\n",
    "product = input(\"Enter the product you want to search\")  #Ask the user for input\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in\")  #Navigate to Amazon Website\n",
    "\n",
    "search_box = driver.find_element(\"id\", 'twotabsearchtextbox')\n",
    "search_box.send_keys(product)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "product_name = []\n",
    "product_price = []\n",
    "product_availability = []\n",
    "product_expected_delivery = []\n",
    "product_link = []\n",
    "\n",
    "items = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, \"s-result-item s-asin\")]')))\n",
    "\n",
    "\n",
    "for item in items:\n",
    "   \n",
    "   try:\n",
    "      name = item.find_element(By.XPATH, './/span[@class=\"a-size-base-plus a-color-base a-text-normal\"]').text\n",
    "      product_name.append(name)\n",
    "   except NoSuchElementException:\n",
    "        product_name.append(\"N/A\")\n",
    "   \n",
    "   try:\n",
    "      price = item.find_element(By.XPATH, './/span[@class=\"a-price-whole\"]').text\n",
    "      product_price.append(price)\n",
    "   except NoSuchElementException:\n",
    "        product_price.append(\"N/A\")\n",
    "   \n",
    "   try:\n",
    "      url = item.find_element(By.XPATH, './/a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]').get_attribute('href')\n",
    "      product_link.append(url)\n",
    "   except NoSuchElementException:\n",
    "        product_link.append(\"N/A\")\n",
    "\n",
    "   \n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(product_name)\n",
    "print(product_price)\n",
    "print(product_link)\n",
    "\n",
    "with open('amazon_results.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Name\", \"Product Price\", \"Product Link\"])\n",
    "    writer.writerows(zip(product_name, product_price, product_link))\n",
    "\n",
    "print(\"Results saved to amazon_results.csv file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d10f4b6",
   "metadata": {},
   "source": [
    "## Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b72f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Define the keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://images.google.com/\")  # Navigate to images.google.com\n",
    "\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Wait for the search bar to load\n",
    "    search_bar = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, './/textarea[@class=\"gLFyf\"]')))\n",
    "    # Clear the search bar and enter the keyword\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the search results to load\n",
    "    search_results = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located(\n",
    "        (By.XPATH, './/img[@class=\"rg_i Q4LuWd\"]')))\n",
    "\n",
    "    # Scrape and download 10 images for the current keyword\n",
    "    for i in range(10):\n",
    "        image = search_results[i]\n",
    "        image_url = image.get_attribute(\"src\")\n",
    "        urllib.request.urlretrieve(\n",
    "            image_url, f\"images/{keyword}_{i+1}.jpg\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Images downloaded successfully.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35bb5458",
   "metadata": {},
   "source": [
    "## Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe to store the scraped details\n",
    "df = pd.DataFrame(columns=[\"Brand Name\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\", \"Primary Camera\",\n",
    "                           \"Secondary Camera\", \"Display Size\", \"Battery Capacity\", \"Price\", \"Product URL\"])\n",
    "\n",
    "# Ask the user for input\n",
    "product = input(\"Enter the smartphone you want to search: \")\n",
    "\n",
    "# Create a new Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Flipkart website\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Close the login popup if it appears\n",
    "try:\n",
    "    close_button = WebDriverWait(driver, 100).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//button[@class=\"_2KpZ6l _2doB4z\"]')))\n",
    "    close_button.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Find the search bar and enter the product name\n",
    "search_bar = WebDriverWait(driver, 100).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//input[@title=\"Search for products, brands and more\"]')))\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(product)\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 100).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"_2kHMtA\"]')))\n",
    "\n",
    "# Get the list of search result items\n",
    "search_results = driver.find_elements(By.XPATH, '//div[@class=\"_2kHMtA\"]')\n",
    "\n",
    "# Iterate through each search result item and scrape the details\n",
    "for result in search_results:\n",
    "    try:\n",
    "        # Get the product URL\n",
    "        product_url = result.find_element(By.XPATH, './/a').get_attribute(\"href\")\n",
    "    except:\n",
    "        product_url = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone brand name and name\n",
    "        brand_name, smartphone_name = result.find_element(By.XPATH, './/div[1]/div[1]/div[1]').text.split(\" \")\n",
    "    except:\n",
    "        brand_name, smartphone_name = \"-\", \"-\"\n",
    "\n",
    "    #try:\n",
    "        # Get the smartphone color\n",
    "        #colour = result.find_element(By.XPATH, './/div[1]/div[1]/div[1]').text.split(\": \")[1]\n",
    "    #except:\n",
    "        #colour = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone RAM\n",
    "        ram = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[1]').text.split(\"|\")[0]\n",
    "    except:\n",
    "        ram = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone storage\n",
    "        storage = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[1]').text.split(\"|\")[1]\n",
    "    except:\n",
    "        storage = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone primary camera\n",
    "        primary_camera = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[3]').text\n",
    "    except:\n",
    "        primary_camera = \"-\"\n",
    "\n",
    "    #try:\n",
    "        # Get the smartphone secondary camera\n",
    "        #secondary_camera = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[4]').text\n",
    "    #except:\n",
    "        #secondary_camera = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone display size\n",
    "        display_size = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[2]').text\n",
    "    except:\n",
    "        display_size = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone battery capacity\n",
    "        battery_capacity = result.find_element(By.XPATH, './/div[1]/div[2]/ul/li[4]').text\n",
    "    except:\n",
    "        battery_capacity = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the smartphone price\n",
    "        price = result.find_element(By.XPATH, './/div[1]/div[3]/div[1]').text\n",
    "    except:\n",
    "        price = \"-\"\n",
    "\n",
    "    # Append the scraped details to the dataframe\n",
    "    df = df.append({\"Brand Name\": brand_name, \"Smartphone Name\": smartphone_name, \"Colour\": colour, \"RAM\": ram,\n",
    "                    \"Storage(ROM)\": storage, \"Primary Camera\": primary_camera, \"Secondary Camera\": secondary_camera,\n",
    "                    \"Display Size\": display_size, \"Battery Capacity\": battery_capacity, \"Price\": price,\n",
    "                    \"Product URL\": product_url}, ignore_index=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "# Close the Chrome driver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed and data saved to flipkart_smartphones.csv.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74f87333",
   "metadata": {},
   "source": [
    "## Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d176b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75a0d1e",
   "metadata": {},
   "source": [
    "## Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5dd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure the Selenium webdriver\n",
    "driver = webdriver.Chrome()  # Use the appropriate webdriver for your browser\n",
    "driver.maximize_window()\n",
    "\n",
    "# Navigate to digit.in website\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "\n",
    "# Click on the \"Top 10\" menu\n",
    "top_10_menu = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[@href=\"https://www.digit.in/top-products/top-10-laptops-5.html\"]')))\n",
    "top_10_menu.click()\n",
    "\n",
    "# Scroll to the \"Best Gaming Laptops\" section\n",
    "gaming_laptops_section = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@id=\"laptops\"]/div[@class=\"TopNumbeHeading active sticky-hl\"]')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", gaming_laptops_section)\n",
    "\n",
    "# Scrape the details of gaming laptops\n",
    "laptop_elements = driver.find_elements(By.XPATH, '//div[@id=\"laptops\"]/div[@class=\"block-styling\"]')\n",
    "laptops_data = []\n",
    "\n",
    "for laptop_element in laptop_elements:\n",
    "    laptop_data = {}\n",
    "    try:\n",
    "        # Get the laptop name\n",
    "        laptop_name = laptop_element.find_element(By.XPATH, './/div[@class=\"heading-wraper\"]/h3').text\n",
    "        laptop_data[\"Laptop Name\"] = laptop_name\n",
    "    except:\n",
    "        laptop_data[\"Laptop Name\"] = \"-\"\n",
    "    \n",
    "    try:\n",
    "        # Get the laptop price\n",
    "        laptop_price = laptop_element.find_element(By.XPATH, './/td[@class=\"smprice\"]').text\n",
    "        laptop_data[\"Price\"] = laptop_price\n",
    "    except:\n",
    "        laptop_data[\"Price\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop operating system\n",
    "        laptop_os = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[1]/div[2]').text\n",
    "        laptop_data[\"Operating System\"] = laptop_os\n",
    "    except:\n",
    "        laptop_data[\"Operating System\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop processor\n",
    "        laptop_processor = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[2]/div[2]').text\n",
    "        laptop_data[\"Processor\"] = laptop_processor\n",
    "    except:\n",
    "        laptop_data[\"Processor\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop RAM\n",
    "        laptop_ram = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[3]/div[2]').text\n",
    "        laptop_data[\"RAM\"] = laptop_ram\n",
    "    except:\n",
    "        laptop_data[\"RAM\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop storage\n",
    "        laptop_storage = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[4]/div[2]').text\n",
    "        laptop_data[\"Storage\"] = laptop_storage\n",
    "    except:\n",
    "        laptop_data[\"Storage\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop display size\n",
    "        laptop_display_size = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[5]/div[2]').text\n",
    "        laptop_data[\"Display Size\"] = laptop_display_size\n",
    "    except:\n",
    "        laptop_data[\"Display Size\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop graphics card\n",
    "        laptop_graphics_card = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[6]/div[2]').text\n",
    "        laptop_data[\"Graphics Card\"] = laptop_graphics_card\n",
    "    except:\n",
    "        laptop_data[\"Graphics Card\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop battery life\n",
    "        laptop_battery_life = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[7]/div[2]').text\n",
    "        laptop_data[\"Battery Life\"] = laptop_battery_life\n",
    "    except:\n",
    "        laptop_data[\"Battery Life\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop weight\n",
    "        laptop_weight = laptop_element.find_element(By.XPATH, './/div[@class=\"prod-spec\"]/div[8]/div[2]').text\n",
    "        laptop_data[\"Weight\"] = laptop_weight\n",
    "    except:\n",
    "        laptop_data[\"Weight\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop overall rating\n",
    "        laptop_rating = laptop_element.find_element(By.XPATH, './/div[@class=\"rating-stars\"]').get_attribute(\"data-rating\")\n",
    "        laptop_data[\"Rating\"] = laptop_rating\n",
    "    except:\n",
    "        laptop_data[\"Rating\"] = \"-\"\n",
    "\n",
    "    try:\n",
    "        # Get the laptop product URL\n",
    "        laptop_url = laptop_element.find_element(By.XPATH, './/div[@class=\"product-description\"]/a').get_attribute(\"href\")\n",
    "        laptop_data[\"Product URL\"] = laptop_url\n",
    "    except:\n",
    "        laptop_data[\"Product URL\"] = \"-\"\n",
    "\n",
    "        laptops_data.append(laptop_data)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(laptops_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"gaming_laptops_digit.csv\", index=False)\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab63e723",
   "metadata": {},
   "source": [
    "## Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a940e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a web driver instance (in this case, using Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Forbes Billionaires page\n",
    "driver.get('https://www.forbes.com/billionaires/')\n",
    "\n",
    "# Wait for the page to load and the billionaire details to be visible\n",
    "wait = WebDriverWait(driver, 20)\n",
    "wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.rank')))\n",
    "\n",
    "# Extract the details of all billionaires\n",
    "billionaire_elements = driver.find_elements(By.CSS_SELECTOR, '.rank')\n",
    "\n",
    "# Create lists to store the details\n",
    "ranks = []\n",
    "names = []\n",
    "net_worths = []\n",
    "ages = []\n",
    "citizenships = []\n",
    "sources = []\n",
    "industries = []\n",
    "\n",
    "# Loop through the billionaire elements and extract the details\n",
    "for element in billionaire_elements:\n",
    "    rank = element.find_element(By.CSS_SELECTOR, '.rank').text\n",
    "    name = element.find_element(By.XPATH, '//div[@class=\"Table_dataCell__2QCve\"]').text\n",
    "    net_worth = element.find_element(By.CSS_SELECTOR, '.netWorth').text\n",
    "    age = element.find_element(By.CSS_SELECTOR, '.age').text\n",
    "    citizenship = element.find_element(By.CSS_SELECTOR, '.countryOfCitizenship').text\n",
    "    source = element.find_element(By.CSS_SELECTOR, '.source-column').text\n",
    "    industry = element.find_element(By.CSS_SELECTOR, '.category').text\n",
    "\n",
    "    # Append the details to the respective lists\n",
    "    ranks.append(rank)\n",
    "    names.append(name)\n",
    "    net_worths.append(net_worth)\n",
    "    ages.append(age)\n",
    "    citizenships.append(citizenship)\n",
    "    sources.append(source)\n",
    "    industries.append(industry)\n",
    "\n",
    "# Create a dictionary to store the scraped data\n",
    "billionaires_data = {\n",
    "    'Rank': ranks,\n",
    "    'Name': names,\n",
    "    'Net worth': net_worths,\n",
    "    'Age': ages,\n",
    "    'Citizenship': citizenships,\n",
    "    'Source': sources,\n",
    "    'Industry': industries\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the web driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e586e85",
   "metadata": {},
   "source": [
    "## Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d566918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa0a7e04",
   "metadata": {},
   "source": [
    "## Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()  \n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Search for London as the location\n",
    "search_input = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@class='native-input body-1-reg']\")))\n",
    "search_input.send_keys(\"London\")\n",
    "search_button = wait.until(EC.presence_of_element_located((By.XPATH, \"//button[@id='search-button']\")))\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results page to load\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"property-card\"]')))\n",
    "\n",
    "# Extract data from each hostel\n",
    "hostels = []\n",
    "property_cards = driver.find_elements(By.XPATH, '//div[@class=\"property-card\"]')\n",
    "for property_card in property_cards:\n",
    "    hostel = {}\n",
    "    hostel[\"name\"] = property_card.find_element(By.XPATH, './/h2').text\n",
    "    hostel[\"distance_from_city_centre\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__distance\"]//span[@class=\"property-card__distance__badge\"]')\\\n",
    "        .text.replace(\"km from city centre\", \"\")\n",
    "    hostel[\"ratings\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__reviews\"]//div[@class=\"score orange\"]')\\\n",
    "        .text\n",
    "    hostel[\"total_reviews\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__reviews\"]//div[@class=\"reviews\"]')\\\n",
    "        .text.replace(\" Total Reviews\", \"\")\n",
    "    hostel[\"overall_reviews\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__reviews\"]//div[@class=\"keyword\"]')\\\n",
    "        .text.replace(\" Overall\", \"\")\n",
    "    hostel[\"privates_from_price\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__pricing\"]//span[@class=\"price price--strikethrough\"]')\\\n",
    "        .text.replace(\" Privates From \", \"\")\n",
    "    hostel[\"dorms_from_price\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__pricing\"]//span[@class=\"price\"]')\\\n",
    "        .text.replace(\" Dorms From \", \"\")\n",
    "    hostel[\"facilities\"] = [facility.text for facility in property_card.find_elements(By.XPATH, './/ul[@class=\"facilities\"]/li')]\n",
    "    hostel[\"property_description\"] = property_card.find_element(By.XPATH, './/div[@class=\"property-card__content\"]')\\\n",
    "        .text.split(\"\\n\")[0]\n",
    "    hostels.append(hostel)\n",
    "\n",
    "# Print the scraped data\n",
    "for hostel in hostels:\n",
    "    print(\"Hostel Name:\", hostel[\"name\"])\n",
    "    print(\"Distance from City Centre:\", hostel[\"distance_from_city_centre\"], \"km\")\n",
    "    print(\"Ratings:\", hostel[\"ratings\"])\n",
    "    print(\"Total Reviews:\", hostel[\"total_reviews\"])\n",
    "    print(\"Overall Reviews:\", hostel[\"overall_reviews\"])\n",
    "    print(\"Privates from Price:\", hostel[\"privates_from_price\"])\n",
    "    print(\"Dorms from Price:\", hostel[\"dorms_from_price\"])\n",
    "    print(\"Facilities:\", hostel[\"facilities\"])\n",
    "    print(\"Property Description:\", hostel[\"property_description\"])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c10d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
