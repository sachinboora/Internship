{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2f3d5b6",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING ASSIGNMENT 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14aae6a8",
   "metadata": {},
   "source": [
    "## Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936daafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install webdriver-manager\n",
    "%pip install selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver         #we use selenium for automation of the programs\n",
    "import pandas as pd                        #to read the dataframe into csv\n",
    "import csv                                     #to import the data into csv\n",
    "from selenium.webdriver.common.keys import Keys                                                            #we need all these modules for all question so imported in 1st line\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from time import sleep        #time we need for website to load\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ffc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Taking user input for product to be searched\n",
    "product = input(\"Enter the product you want to search: \")\n",
    "\n",
    "# Setting up the Chrome driver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# Opening the Amazon website\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "# Finding the search box and inputting the user input\n",
    "search_box = driver.find_element_by_xpath('//input[@type=\"text\" and @name=\"field-keywords\"]')\n",
    "search_box.send_keys(product)\n",
    "\n",
    "# Finding the search button and clicking it\n",
    "search_button = driver.find_element_by_xpath('//input[@type=\"submit\" and @value=\"Go\"]')\n",
    "search_button.click()\n",
    "\n",
    "# Creating lists to store the details of each product\n",
    "brand = []\n",
    "name = []\n",
    "price = []\n",
    "return_exchange = []\n",
    "expected_delivery = []\n",
    "availability = []\n",
    "url = []\n",
    "\n",
    "# Looping through the first 3 pages of search results\n",
    "for i in range(3):\n",
    "    # Finding all the products on the current page\n",
    "    products = driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]')\n",
    "    \n",
    "    # Looping through each product on the current page\n",
    "    for product in products:\n",
    "        try:\n",
    "            # Extracting the details of the product\n",
    "            brand_name = product.find_element_by_xpath('.//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]')\n",
    "            brand.append(brand_name.text)\n",
    "        except:\n",
    "            brand.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_name = product.find_element_by_xpath('.//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "            name.append(product_name.text)\n",
    "        except:\n",
    "            name.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_price = product.find_element_by_xpath('.//span[@class=\"a-price-whole\"]')\n",
    "            price.append(product_price.text)\n",
    "        except:\n",
    "            price.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_return_exchange = product.find_element_by_xpath('.//span[contains(text(), \"Return or Exchange\")]')\n",
    "            return_exchange.append(product_return_exchange.text)\n",
    "        except:\n",
    "            return_exchange.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_expected_delivery = product.find_element_by_xpath('.//span[@class=\"a-text-bold\"]')\n",
    "            expected_delivery.append(product_expected_delivery.text)\n",
    "        except:\n",
    "            expected_delivery.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_availability = product.find_element_by_xpath('.//span[@class=\"a-size-base a-color-success\"]')\n",
    "            availability.append(product_availability.text)\n",
    "        except:\n",
    "            availability.append('-')\n",
    "            \n",
    "        try:\n",
    "            product_url = product.find_element_by_xpath('.//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "            url.append(product_url.get_attribute('href'))\n",
    "        except:\n",
    "            url.append('-')\n",
    "    \n",
    "    # Checking if there is a next page of search results\n",
    "    try:\n",
    "        next_page = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')\n",
    "        driver.get(next_page.get_attribute('href'))\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Creating a dictionary of the product details\n",
    "product_dict = {'Brand Name': brand, 'Name of the Product': name, 'Price': price, 'Return/Exchange': return_exchange,\n",
    "                'Expected Delivery': expected_delivery, 'Availability': availability, 'Product URL': url}\n",
    "\n",
    "# Creating a data frame from the dictionary\n",
    "product_df = pd.DataFrame(product_dict)\n",
    "\n",
    "# Saving the data frame to a CSV file\n",
    "product_df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "# Closing the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e5a08e",
   "metadata": {},
   "source": [
    "## In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b63027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract product details\n",
    "def extract_product_details(product):\n",
    "    try:\n",
    "        # Extracting product details\n",
    "        product_url = product.find_element_by_tag_name('a').get_attribute('href')\n",
    "        product_brand = product.find_element_by_class_name('s-line-clamp-1').text\n",
    "        product_name = product.find_element_by_class_name('a-size-medium').text\n",
    "        product_price = product.find_element_by_class_name('a-price-whole').text\n",
    "        product_return_exchange = product.find_element_by_class_name('s-prime').text\n",
    "        product_delivery = product.find_element_by_class_name('s-align-children-center').text\n",
    "        product_availability = product.find_element_by_class_name('s-nowrap').text\n",
    "    except NoSuchElementException:\n",
    "        # If any of the details are missing for any of the product\n",
    "        product_url = '-'\n",
    "        product_brand = '-'\n",
    "        product_name = '-'\n",
    "        product_price = '-'\n",
    "        product_return_exchange = '-'\n",
    "        product_delivery = '-'\n",
    "        product_availability = '-'\n",
    "        \n",
    "    return [product_brand, product_name, product_price, product_return_exchange, product_delivery, product_availability, product_url]\n",
    "\n",
    "# Taking user input\n",
    "user_input = input(\"Enter the product name to search: \")\n",
    "\n",
    "# Initializing ChromeDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "\n",
    "# Navigating to Amazon.in and searching for the product\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_box.send_keys(user_input)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Creating empty list to store product details\n",
    "products_list = []\n",
    "\n",
    "# Extracting details of products from the first 3 pages of the search results\n",
    "for i in range(1, 4):\n",
    "    # Getting product containers from the search result page\n",
    "    product_containers = driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]')\n",
    "    for product in product_containers:\n",
    "        # Extracting product details and appending to the products_list\n",
    "        product_details = extract_product_details(product)\n",
    "        products_list.append(product_details)\n",
    "    try:\n",
    "        # Clicking the \"Next\" button to move to the next page\n",
    "        next_button = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')\n",
    "        next_button.click()\n",
    "        sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        # If the \"Next\" button is not present, break the loop\n",
    "        break\n",
    "\n",
    "# Saving the product details in a CSV file\n",
    "header = [\"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Product URL\"]\n",
    "with open(\"amazon_products.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(products_list)\n",
    "\n",
    "# Converting the product details into a pandas dataframe\n",
    "df = pd.DataFrame(products_list, columns=header)\n",
    "\n",
    "# Displaying the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Closing the ChromeDriver\n",
    "driver.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d10f4b6",
   "metadata": {},
   "source": [
    "## Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b72f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify the path of chromedriver.exe\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# get the google images webpage\n",
    "driver.get('https://images.google.com/')\n",
    "\n",
    "# function to download image\n",
    "def download_image(img_url, file_name):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(img_url, file_name)\n",
    "        print(\"Image downloaded: \", file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error while downloading image: \", e)\n",
    "\n",
    "# function to search and download images for a given keyword\n",
    "def search_and_download(keyword):\n",
    "    # locate the search bar and enter the keyword\n",
    "    search_bar = driver.find_element_by_name('q')\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    # locate the search button and click it\n",
    "    search_button = driver.find_element_by_css_selector('.Tg7LZd')\n",
    "    search_button.click()\n",
    "\n",
    "    # scroll down to load more images\n",
    "    for i in range(2):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # get the image urls and download first 10 images\n",
    "    images = driver.find_elements_by_css_selector('.rg_i')\n",
    "    count = 1\n",
    "    for image in images[:10]:\n",
    "        img_url = image.get_attribute('src')\n",
    "        if img_url and 'http' in img_url:\n",
    "            file_name = os.path.join(keyword, keyword + \"_\" + str(count) + \".jpg\")\n",
    "            download_image(img_url, file_name)\n",
    "            count += 1\n",
    "\n",
    "# list of keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# search and download images for each keyword\n",
    "for keyword in keywords:\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "    search_and_download(keyword)\n",
    "\n",
    "# close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35bb5458",
   "metadata": {},
   "source": [
    "## Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.flipkart.com/search?q=oneplus+nord&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "brands = []\n",
    "names = []\n",
    "colors = []\n",
    "rams = []\n",
    "roms = []\n",
    "primary_cameras = []\n",
    "secondary_cameras = []\n",
    "display_sizes = []\n",
    "battery_capacities = []\n",
    "prices = []\n",
    "product_urls = []\n",
    "\n",
    "# Find all the div elements with class '_1YokD2 _3Mn1Gg'\n",
    "products = soup.find_all('div', {'class': '_1YokD2 _3Mn1Gg'})\n",
    "\n",
    "# Loop through each product and extract the required information\n",
    "for product in products:\n",
    "    try:\n",
    "        # Extract the brand name\n",
    "        brand = product.find('div', {'class': '_2kHMtA'}).text\n",
    "    except:\n",
    "        brand = '-'\n",
    "    try:\n",
    "        # Extract the smartphone name\n",
    "        name = product.find('a', {'class': '_1fQZEK'}).text\n",
    "    except:\n",
    "        name = '-'\n",
    "    try:\n",
    "        # Extract the color\n",
    "        color = product.find('div', {'class': '_2kHMtA'}).next_sibling.text\n",
    "    except:\n",
    "        color = '-'\n",
    "    try:\n",
    "        # Extract the RAM\n",
    "        ram = product.find('li', {'class': 'rgWa7D'}).text\n",
    "    except:\n",
    "        ram = '-'\n",
    "    try:\n",
    "        # Extract the ROM\n",
    "        rom = product.find('li', {'class': 'rgWa7D'}).next_sibling.text\n",
    "    except:\n",
    "        rom = '-'\n",
    "    try:\n",
    "        # Extract the primary camera\n",
    "        primary_camera = product.find('li', {'class': 'rgWa7D'}).next_sibling.next_sibling.text\n",
    "    except:\n",
    "        primary_camera = '-'\n",
    "    try:\n",
    "        # Extract the secondary camera\n",
    "        secondary_camera = product.find('li', {'class': 'rgWa7D'}).next_sibling.next_sibling.next_sibling.text\n",
    "    except:\n",
    "        secondary_camera = '-'\n",
    "    try:\n",
    "        # Extract the display size\n",
    "        display_size = product.find('li', {'class': 'rgWa7D'}).next_sibling.next_sibling.next_sibling.next_sibling.text\n",
    "    except:\n",
    "        display_size = '-'\n",
    "    try:\n",
    "        # Extract the battery capacity\n",
    "        battery_capacity = product.find('li', {'class': 'rgWa7D'}).next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.text\n",
    "    except:\n",
    "        battery_capacity = '-'\n",
    "    try:\n",
    "        # Extract the price\n",
    "        price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "    except:\n",
    "        price = '-'\n",
    "    try:\n",
    "        # Extract the product URL\n",
    "        product_url = 'https://www.flipkart.com' + product.find('a', {'class': '_1fQZEK'})['href']\n",
    "    except:\n",
    "        product_url = '-'\n",
    "\n",
    "    # Append the data to the lists\n",
    "    brands.append(brand)\n",
    "    names.append(name)\n",
    "    colors.append(color)\n",
    "    rams.append(ram)\n",
    "    roms.append(rom)\n",
    "    primary_cameras.append(primary_camera)\n",
    "    secondary_cameras.append(secondary_camera)\n",
    "    display_sizes.append(display_size)\n",
    "    battery_capacities.append(battery_capacity)\n",
    "    prices.append(price)\n",
    "\n",
    "# adding the data to a dataframe\n",
    "pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Name': names,\n",
    "    'Color': colors,\n",
    "    'RAM': rams,\n",
    "    'ROM': roms,\n",
    "    'Primary_Camera': primary_cameras,\n",
    "    'Secondary_Camera': secondary_cameras,\n",
    "    'Display_Size': display_sizes,\n",
    "    'Battery_Capacity': battery_capacities,\n",
    "    'Price': prices})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74f87333",
   "metadata": {},
   "source": [
    "### Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "city_name = input(\"Enter the name of the city: \")\n",
    "\n",
    "url = \"https://www.latlong.net/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the search form on the page\n",
    "search_form = soup.find('form', attrs={'class': 'searchbox'})\n",
    "\n",
    "# find the search input field and set its value to the city name\n",
    "search_input = search_form.find('input', attrs={'name': 'place'})\n",
    "search_input['value'] = city_name\n",
    "\n",
    "# submit the form and get the search result page\n",
    "submit_button = search_form.find('button')\n",
    "search_url = url + submit_button['formaction']\n",
    "response = requests.post(search_url, data=search_form.form_data())\n",
    "\n",
    "# find the latitude and longitude on the search result page\n",
    "search_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "latlong_div = search_soup.find('div', attrs={'class': 'latlong'})\n",
    "\n",
    "if latlong_div is None:\n",
    "    print(\"Sorry, we could not find the coordinates for\", city_name)\n",
    "else:\n",
    "    latitude, longitude = latlong_div.text.strip().split(',')\n",
    "    print(\"The coordinates of\", city_name, \"are\", latitude.strip(), \"latitude and\", longitude.strip(), \"longitude.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd647e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
